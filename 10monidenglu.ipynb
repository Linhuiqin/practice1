{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92b4f71-1d4d-4926-bbda-979a766b9120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'目前比较流行的实现方法有两种，一个是基于session和cookie的验证，一种是基于jwt（json web token）\\n的验证'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模拟登陆\n",
    "'''目前比较流行的实现方法有两种，一个是基于session和cookie的验证，一种是基于jwt（json web token）\n",
    "的验证'''\n",
    "\n",
    "#session就是存在服务端的，保存了用户此次访问的会话信息。\n",
    "#cookie则是保存在用户本地浏览器的，它会在每次用户访问网站的时候作为请求头发送给服务器，\n",
    "#服务器会从中找出session对象并判断起访问用户的状态最后返回response内容\n",
    "#方法一，把cookie放到代码中，每次爬虫请求时再把它放到请求头中\n",
    "#方法二，如果不想有任何手工操作，就直接用爬虫模拟登陆过程，即post请求，直接爬虫提交用户名\n",
    "#密码给服务器，服务器放回的response headers里面可能会有set cookie字段，只需把这些cookie保存下来\n",
    "#所以这个过程最重要的是维持好cookie。常遇到的问题，登陆过程伴随着各种校验参数，不好直接模拟请求；\n",
    "#网站设置cookie的时候是通过javascript实现，所以需要仔细分析其中逻辑，尤其是用requests这样的请求库\n",
    "#进行模拟登陆的时候，遇到的问题经常比较多\n",
    "#方法三，用selenium，puppeteer或playwright来实现登陆过程的自动化\n",
    "#总之，每次请求都携带好cookie信息就能实现模拟登陆里\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b59f96-7263-4e09-b0b9-850471a6e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "Cookies <RequestsCookieJar[<Cookie sessionid=j6c1h3qrll49i78bnzo3ceugdvnuvjx3 for login2.scrape.center/>]>\n",
      "Response Status 200\n",
      "Response URL https://login2.scrape.center/page/1\n"
     ]
    }
   ],
   "source": [
    "#打开开发者工具，观察账号密码登陆时服务器发生的请求。\n",
    "#登陆的瞬间，浏览器发起了一个post请求，目标URL为https://login2.scrape.center/login\n",
    "#同时还提交了用户名和密码这两个数据。提交完成后，返回状态码为302，同时在set cookie里面有session ID\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = 'https://login2.scrape.center/'\n",
    "LOGIN_URL = urljoin(BASE_URL, '/login')\n",
    "INDEX_URL = urljoin(BASE_URL, '/page/1')\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = 'admin'\n",
    "\n",
    "response_login = requests.post(LOGIN_URL, data={\n",
    "    'username': USERNAME,\n",
    "    'password': PASSWORD\n",
    "}, allow_redirects=False)#把重定向关掉\n",
    "print(type(response_login))#是个response类\n",
    "\n",
    "cookies = response_login.cookies#获取其cookie信息\n",
    "print('Cookies', cookies)\n",
    "\n",
    "response_index = requests.get(INDEX_URL, cookies=cookies)\n",
    "print('Response Status', response_index.status_code)\n",
    "print('Response URL', response_index.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37364e3-ca84-4f9c-91b5-aea9add28d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies <RequestsCookieJar[<Cookie sessionid=b38k9h476r1t93jj7ryo0sfuwq5ag0i1 for login2.scrape.center/>]>\n",
      "Response Status 200\n",
      "Response URL https://login2.scrape.center/page/1\n"
     ]
    }
   ],
   "source": [
    "#如果想每次都不用带着登陆时获取的cookie请求，来请求接下来的每个页面，可以直接用session\n",
    "#直接用session类代替requests\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = 'https://login2.scrape.center/'\n",
    "LOGIN_URL = urljoin(BASE_URL, '/login')\n",
    "INDEX_URL = urljoin(BASE_URL, '/page/1')\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = 'admin'\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "response_login = session.post(LOGIN_URL, data={\n",
    "    'username': USERNAME,\n",
    "    'password': PASSWORD\n",
    "})\n",
    "\n",
    "cookies = session.cookies\n",
    "print('Cookies', cookies)\n",
    "\n",
    "response_index = session.get(INDEX_URL)\n",
    "print('Response Status', response_index.status_code)\n",
    "print('Response URL', response_index.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fafc12d-a86c-4e84-ae68-158671b357d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_css_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m browser \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\u001b[38;5;66;03m#用selenium先打开了chrome\u001b[39;00m\n\u001b[1;32m     18\u001b[0m browser\u001b[38;5;241m.\u001b[39mget(BASE_URL)\u001b[38;5;66;03m#跳转到登陆页面。下面是模拟输入了用户名和密码，\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element_by_css_selector\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msend_keys(USERNAME)\n\u001b[1;32m     20\u001b[0m browser\u001b[38;5;241m.\u001b[39mfind_element_by_css_selector(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msend_keys(PASSWORD)\n\u001b[1;32m     21\u001b[0m browser\u001b[38;5;241m.\u001b[39mfind_element_by_css_selector(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput[type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\u001b[38;5;66;03m#这里是点击了登录按钮\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_css_selector'"
     ]
    }
   ],
   "source": [
    "#如果碰到难以模拟登陆的过程（比如带有验证码，带有加密参数等情况），就可以用selenium等模拟登陆目的是获取登陆后的cookie，\n",
    "#获取到cookie后，和前面一样用这些cookie爬取其他页面就好\n",
    "\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    "\n",
    "BASE_URL = 'https://login2.scrape.center/'\n",
    "LOGIN_URL = urljoin(BASE_URL, '/login')\n",
    "INDEX_URL = urljoin(BASE_URL, '/page/1')\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = 'admin'\n",
    "\n",
    "browser = webdriver.Chrome()#用selenium先打开了chrome\n",
    "browser.get(BASE_URL)#跳转到登陆页面。下面是模拟输入了用户名和密码，\n",
    "browser.find_element_by_css_selector('input[name=\"username\"]').send_keys(USERNAME)\n",
    "browser.find_element_by_css_selector('input[name=\"password\"]').send_keys(PASSWORD)\n",
    "browser.find_element_by_css_selector('input[type=\"submit\"]').click()#这里是点击了登录按钮\n",
    "time.sleep(10)\n",
    "\n",
    "# get cookies from selenium\n",
    "cookies = browser.get_cookies()#获取当前浏览器的所有cookie。用这些就能访问其他数据了\n",
    "print('Cookies', cookies)\n",
    "browser.close()\n",
    "\n",
    "# set cookies to requests\n",
    "session = requests.Session()#声明了requests的session对象\n",
    "for cookie in cookies:#遍历刚才的cookie并将其设置到session对象的cookie属性上\n",
    "    session.cookies.set(cookie['name'], cookie['value'])\n",
    "\n",
    "response_index = session.get(INDEX_URL)#接着再拿这个session对象去请求INDEX_URL，就能够直接获取页面而不会跳转到登陆页面了\n",
    "print('Response Status', response_index.status_code)\n",
    "print('Response URL', response_index.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d9c65-87cf-4726-b5ce-edea756628a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1b8b1-69b8-4747-b1a5-7f1cd26b2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多线程学习https://cuiqingcai.com/202271.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d0588-ed39-456b-a147-83eeecdb2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#账号池\n",
    "#如果爬虫要求爬取的数据量比较大或爬取速度比较快，而网站又有单账号并发限制或访问状态检测\n",
    "#等反爬虫手段，那我们等账号可能就会无法访问或者面临封号分险\n",
    "#这种情况下，我们可以使用分流等方案来实现。假设某个网站设置一分钟之内检测到同一个账号访问\n",
    "#三次获三次以上则封号。我们就可以建立一个账号池，用多个账号来随机访问或爬取数据，这样就能大幅度提高\n",
    "#爬虫的并发量，降低被封号的风险。\n",
    "#比如我们可以准备100个账号，然后100个账号都模拟登陆，把对应的cookie或者jwt存下来，每次访问的时候\n",
    "#随机取一个，由于账号多，所以每个账号被取用的概率也就降下来了，也就降低封号风险"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
